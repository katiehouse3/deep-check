{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCheck - Smarter Gun Background Checks\n",
    "Introducing a smart and robust background check.\n",
    "\n",
    "## The Current Process of U.S. Firearm Checks\n",
    "1) Firearm Buyer: Fills out an ATF Form 4473 with:`name`, `age`, `address`, `place of birth`, `race`,  `citizenship`, `Social Security (optional)`, as well as the following questions:\n",
    "  - Have you ever been convicted of a felony?\n",
    "  - Have you ever been convicted of a misdemeanor crime of domestic violence?\n",
    "  - Are you an unlawful user of, or addicted to, marijuana or any other depressant, stimulant, narcotic drug, or any other controlled substance?\n",
    "  - Are you a fugitive from justice?\n",
    "  - Have you ever been committed to a mental institution?\n",
    "\n",
    "2) Firearm Seller: Submits the information to the FBI via a toll-free phone line or over the internet, and the agency checks the applicant's info against databases\n",
    "\n",
    "3) FBI: Conducts background check with the submitted form (can take minutes). The FBI will deny a claim to Fire\n",
    "\n",
    "*Source: https://www.cnn.com/2018/02/15/us/gun-background-checks-florida-school-shooting/index.html*\n",
    "\n",
    "\n",
    "## The Purpose of DeepCheck\n",
    "DeepCheck builds upon the current foundational parameters in background checks and introduces the concept of utilizing a candidates' digital interactions on social media to further diagnosis their level of \"at risk\" for improper use with a firearm. The current social media application we have decided to utilize is Twitter. DeepCheck gathers a candidates' recent (up to 500) status updates (or tweets), retweets, and favorites. Upon gathering the data, DeepCheck runs a natural language processing algorithm which implements sentiment analysis to spotlight offensive language, hate speech, and any encouragement of violent crime. The purpose of this in-depth analysis is to catch hidden sentiments or motives in individuals who do not have a past history with crime or law enforcement.\n",
    "\n",
    "## Dataset and Feature Engineering\n",
    "\n",
    "The dataset and feature engineering methods were obtained from the following publication:\n",
    "\n",
    "Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. \"Automated Hate Speech Detection and the Problem of Offensive Language.\" Proceedings of the 11th International Conference on Web and Social Media (ICWSM).\n",
    "\n",
    "Git Repo: https://github.com/t-davidson/hate-speech-and-offensive-language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# IMPORT AZURE LIBRARIES\n",
    "# Azure Notebook Libraries\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "import logging\n",
    "\n",
    "# IMPORT DATA SCIENCE LIBRARIES\n",
    "import pandas as pd \n",
    "import scipy\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Azure Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\house\\Documents\\GitHub\\config.json\n"
     ]
    }
   ],
   "source": [
    "# Load workspace\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the experiment and specify the project folder.\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "experiment_name = 'hatespeech_detection'\n",
    "project_folder = './hatespeech_project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text_data.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class imbalance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1   0.80\n",
       "0   0.10\n",
       "Name: new_class, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print('class imbalance:')\n",
    "df['new_class'].value_counts() / df['class'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this class imbalance, we chose to upsample the class 0 (innoculus tweets) to match class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbanalnced Class Frequencies:\n",
      "1   0.88\n",
      "0   0.12\n",
      "Name: new_class, dtype: float64\n",
      "First count: 24783\n",
      "\n",
      "New Class Frequencies:\n",
      "1   0.50\n",
      "0   0.50\n",
      "Name: new_class, dtype: float64\n",
      "Updated count: 5744\n"
     ]
    }
   ],
   "source": [
    "print('Imbanalnced Class Frequencies:')\n",
    "print(df['new_class'].value_counts() / df['new_class'].count())\n",
    "print('First count: %i' % df['class'].count())\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_0 = df[df['new_class']==0]\n",
    "df_1 = df[df['new_class']==1]\n",
    "\n",
    "# Number of observations in majority class\n",
    "i_class1 = np.where(df['new_class'] == 0)[0]\n",
    "n_class1 = int(round(len(i_class1),0))\n",
    "\n",
    "# Upsample to match classes\n",
    "df_0_upsampled = resample(df_1, \n",
    "                            replace=False,           # sample with replacement\n",
    "                            n_samples=n_class1,     # to match minority class\n",
    "                            random_state=123)       # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_0_upsampled, df_0])\n",
    " \n",
    "# Display new class counts\n",
    "print('\\nNew Class Frequencies:')\n",
    "print(df_downsampled['new_class'].value_counts() / df_downsampled['new_class'].count())\n",
    "print('Updated count: %i' % df_downsampled['new_class'].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extration\n",
    "These data have over 20,000 labeled tweets in this dataset. Most tweets contain special characters and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_downsampled[\"tweet\"]\n",
    "import feature_extraction\n",
    "\n",
    "X, vectorizer, tfidf, pos_vectorizer = feature_extraction.get_features(tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cloud Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data to Datastore\n",
    "The training data must be uploaded to the datastore in order to compute the training on the cloud. To do this, the training data will be downloaded locally to a `.tsv` file and then uploaded to the datastore with the `ds.upload()` command. Uploading to the datastore is a one-time task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the data in a temporary folder\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "# store the get_data() script in the project folder\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test train sets \n",
    "y = df_downsampled[\"new_class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).to_csv(\"data/X_train.tsv\", index=False, header=False, quoting=csv.QUOTE_ALL, sep=\"\\t\")\n",
    "pd.DataFrame(y_train).to_csv(\"data/y_train.tsv\", index=False, header=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ./data\\X_train.tsv\n",
      "Uploading ./data\\y_train.tsv\n",
      "Uploaded ./data\\y_train.tsv, 1 files out of an estimated total of 2\n",
      "Uploaded ./data\\X_train.tsv, 2 files out of an estimated total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_aef24cf01c0e47adb29ebe0d34af7878"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir='./data', target_path='hatespeech_data', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "dr = DataReferenceConfiguration(datastore_name=ds.name, \n",
    "                   path_on_datastore='hatespeech_data', \n",
    "                   path_on_compute='/tmp/azureml_runs',\n",
    "                   mode='download', # download files from datastore to compute target\n",
    "                   overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model with Cloud Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"deepcheck\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "    \n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\",\n",
    "                                                                max_nodes = 6)\n",
    "\n",
    "    # Create the cluster.\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# create a new RunConfig object\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute\n",
    "conda_run_config.target = compute_target\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "conda_run_config.environment.docker.base_image = azureml.core.runconfig.DEFAULT_CPU_IMAGE\n",
    "\n",
    "# set the data reference of the run coonfiguration\n",
    "conda_run_config.data_references = {ds.name: dr}\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]'], conda_packages=['numpy','py-xgboost<=0.80'])\n",
    "conda_run_config.environment.python.conda_dependencies = cd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./hatespeech_project/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_folder/get_data.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_data():\n",
    "    X_train = pd.read_csv(\"/tmp/azureml_runs/hatespeech_data/X_train.tsv\", delimiter=\"\\t\", header=None, quotechar='\"')\n",
    "    y_train = pd.read_csv(\"/tmp/azureml_runs/hatespeech_data/y_train.tsv\", delimiter=\"\\t\", header=None, quotechar='\"')\n",
    "\n",
    "    return { \"X\" : X_train.values, \"y\" : y_train[0].values }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\": 10,\n",
    "    \"iterations\": 10,\n",
    "    \"n_cross_validations\": 3,\n",
    "    \"primary_metric\": 'AUC_weighted',\n",
    "    \"verbosity\": logging.INFO\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             path = project_folder,\n",
    "                             run_configuration=conda_run_config,\n",
    "                             data_script = project_folder + \"/get_data.py\",\n",
    "                             **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on remote compute: deepcheck\n",
      "Parent Run ID: AutoML_c7056a49-84c9-4cb9-a1ff-44e2aed23d34\n",
      "********************************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "SAMPLING %: Percent of the training data to sample.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "********************************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       SAMPLING %  DURATION      METRIC      BEST\n",
      "         0   MaxAbsScaler LightGBM                          100.0000    0:01:20       0.9053    0.9053\n",
      "         1   RobustScaler LightGBM                          100.0000    0:01:19       0.9588    0.9588\n",
      "         2   RobustScaler LogisticRegression                100.0000    0:01:19       0.9655    0.9655\n",
      "         3   StandardScalerWrapper LightGBM                 100.0000    0:01:20       0.8580    0.9655\n",
      "         4   MaxAbsScaler LightGBM                          100.0000    0:01:19       0.9402    0.9655\n",
      "         5   MinMaxScaler LightGBM                          100.0000    0:01:21       0.9195    0.9655\n",
      "         6   StandardScalerWrapper LightGBM                 100.0000    0:01:20       0.9183    0.9655\n",
      "         7   MinMaxScaler LogisticRegression                100.0000    0:01:21       0.9492    0.9655\n",
      "         8   StandardScalerWrapper LightGBM                 100.0000    0:01:19       0.8588    0.9655\n",
      "         9    Ensemble                                      100.0000    0:01:20       0.9717    0.9717\n"
     ]
    }
   ],
   "source": [
    "run = experiment.submit(automl_config, show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>hatespeech_detection</td><td>AutoML_c7056a49-84c9-4cb9-a1ff-44e2aed23d34</td><td>automl</td><td>Completed</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/a989cac4-671d-45c6-9d2b-ea7e9d936600/resourceGroups/AmherstRG/providers/Microsoft.MachineLearningServices/workspaces/AmherstWorkspace/experiments/hatespeech_detection/runs/AutoML_c7056a49-84c9-4cb9-a1ff-44e2aed23d34\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: hatespeech_detection,\n",
       "Id: AutoML_c7056a49-84c9-4cb9-a1ff-44e2aed23d34,\n",
       "Type: automl,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the model with log loss minimized\n",
    "best_run, fitted_model = run.get_output(iteration=None, metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 974, 1: 922}\n",
      "Accuracy score: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Randomly select digits and test.\n",
    "from azureml.core.model import Model\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "predicted = fitted_model.predict(X_test)\n",
    "\n",
    "unique, counts = np.unique(predicted, return_counts=True)\n",
    "\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score: %.2f' % accuracy_score(y_true=y_test, y_pred=predicted))\n",
    "\n",
    "pd.DataFrame(zip(y_test,predicted)).to_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model AutoMLc7056a498best\n"
     ]
    }
   ],
   "source": [
    "model = run.register_model('deepcheck')\n",
    "# Best model is: AutoML54f661779best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Real Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      RT @DuolingoUS: –ï—Å–ª–∏ –≤–∞–º –ø—Ä–∏—à–ª–æ—Å—å –ø–µ—Ä–µ–≤–µ—Å—Ç–∏, —á...\n",
      "1      RT @ericswalwell: Are you ready America? Let's...\n",
      "2      RT @CNN: JUST IN: Democratic Rep. Eric Swalwel...\n",
      "3      RT @BernieSanders: This is a real national eme...\n",
      "4      RT @umasscs: Congratulations to all the winnin...\n",
      "5      RT @mic: The way men send emails vs. women rev...\n",
      "6                                     @saltystvph Not ok\n",
      "7      RT @TDisfromNYC: Sup twitter fam üññüèæ.\\n\\nI'm a ...\n",
      "8      RT @MforMEGAN: @DMetriaT @quenblackwell Here y...\n",
      "9      RT @quenblackwell: pick the llama up....now. h...\n",
      "10     RT @nihilisims: BRO THIS IS THE CUTEST SWEETES...\n",
      "11     RT @RepTedLieu: Dear @POTUS: Your weird belief...\n",
      "12     this is ridiculous that‚Äôs literally a mang tee...\n",
      "13     RT @Jessie46914117: Hey everybody. My momma ha...\n",
      "14     RT @ElaLaineReeves: I shared this yesterday an...\n",
      "15     RT @milkygoddess: WHEN SHE LOOKS AT THE MONITO...\n",
      "16     RT @tabitchaaa: Look what you bitches are doin...\n",
      "17     RT @abby_thatsme: you guys better help this ma...\n",
      "18     RT @INDIEWASHERE: i love elephants they're mos...\n",
      "19                    @kristiekwong WELCOME TO TWEEEETER\n",
      "20     RT @darehgregorian: No more 'kitten slaughterh...\n",
      "21     RT @THEkarliehustle: i was talking to a lyft d...\n",
      "22     RT @hoegenic: maybe if we stopped destroying t...\n",
      "23     RT @nytimes: ‚ÄúThis is a campaign for America, ...\n",
      "24     RT @BetoORourke: It is here, from my hometown ...\n",
      "25     RT @praisedbieber: The tiger is drugged, look ...\n",
      "26     RT @lilmushroomhat: Plastic was invented in 19...\n",
      "27     RT @GirlsWhoCode: Happy Birthday to US! We tur...\n",
      "28     RT @risconrado: see how nice the ocean can be ...\n",
      "29     RT @divblita: PSA to all my ladies! You can ge...\n",
      "                             ...                        \n",
      "168    RT @SpeakerPelosi: During today‚Äôs meeting betw...\n",
      "169    RT @alliyarb8: Worth the read https://t.co/BSh...\n",
      "170    RT @HoneyDipBri: New California Laws effective...\n",
      "171    RT @flowershellbomb: RETWEET IN 10 SECONDS FOR...\n",
      "172    RT @CorbettLakeisha: Everyone should see this ...\n",
      "173    See the good the world searched for in 2018. W...\n",
      "174    RT @jade_maples: PSA for anyone and everyone o...\n",
      "175    RT @TIME: California will become first state t...\n",
      "176    RT @Khushboo_: Saying goodbye to a species, th...\n",
      "177    RT @_beminepls: This photo is a curse if you d...\n",
      "178    RT @rudy_mustang: amazing https://t.co/i3XMeLBp4u\n",
      "179    RT @MileyCyrus: Santa Baby üéÖ @MarkRonson @jimm...\n",
      "180    RT @RepBarbaraLee: To recap, in the past 48 ho...\n",
      "181    RT @RBReich: Trump has shut down the governmen...\n",
      "182    RT @_dianakris: Please do not shame people for...\n",
      "183                         Good https://t.co/i7HM4Li176\n",
      "184    RT @BienSur_JeTaime: This was Cardi's moment. ...\n",
      "185    RT @KamalaHarris: A 7-year-old girl died of de...\n",
      "186    RT @MattxRed: The kids from Sandy Hook should ...\n",
      "187    RT @GregoryMcKelvey: I had a police officer st...\n",
      "188    RT @theEmmyKees: How sad is it that in America...\n",
      "189    RT @SInow: In her first public statement since...\n",
      "190    RT @Ness_Qwikk: *Trigger Warning*\\n\\nJACOB WAL...\n",
      "191    RT @Ocasio2018: Double standards are Paul Ryan...\n",
      "192    RT @bryultra: RT or bad luck on your finals fo...\n",
      "193    RT @ACLU: Cyntoia Brown has been victimized tw...\n",
      "194    RT @owenawhaley: Brock Turner raped somebody a...\n",
      "195    RT @sahluwal: Cyntoia Brown was a 16 year-old ...\n",
      "196             dogs &gt; humans https://t.co/Xq2NUsEbxT\n",
      "197    RT @nowthisnews: China has detained over a mil...\n",
      "Name: tweet, Length: 198, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary package to process data in JSON format\n",
    "try:\n",
    "    import json\n",
    "except ImportError:\n",
    "    import simplejson as json\n",
    "\n",
    "# Import the tweepy library\n",
    "import tweepy\n",
    "import pprint\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "'''\n",
    "The Access token is hidden from open source code\n",
    "# Setup tweepy to authenticate with Twitter credentials:\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "ACCESS_TOKEN = <YOUR ACCESS TOKEN>\n",
    "ACCESS_SECRET = <YOUR ACCESS SECRET>\n",
    "CONSUMER_KEY = <YOUR CUSTOMER KEY>\n",
    "CONSUMER_SECRET = <YOUR CUSTOMER SECRET>s\n",
    "'''\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "# Create the api to connect to twitter with your creadentials\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "status_cursor = tweepy.Cursor(api.user_timeline, screen_name=\"dishsrivastava\", count=1000)\n",
    "status_list = status_cursor.iterator.next()\n",
    "\n",
    "user_tweets = []\n",
    "for i in range(len(status_list)):\n",
    "    user_tweets += [status_list[i].text]\n",
    "\n",
    "user_df = pd.DataFrame(user_tweets,columns=['tweet'])\n",
    "\n",
    "print(user_df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text\n",
    "\n",
    "\n",
    "def transform_inputs(tweets, tf_vectorizer, idf_vector, pos_vectorizer):\n",
    "    tf_array = tf_vectorizer.transform(tweets).toarray()\n",
    "    #print(tf_array.shape)\n",
    "    #tfidf_array = tf_array*idf_vector\n",
    "    #print(\"Built TF-IDF array\")\n",
    "\n",
    "    tweet_tags = get_pos_tags(tweets)\n",
    "    pos_array = pos_vectorizer.transform(pd.Series(tweet_tags)).toarray()\n",
    "    #print(\"Built POS array\")\n",
    "    \n",
    "    oth_array = get_feature_array(tweets)\n",
    "    #print(\"Built other feature array\")\n",
    "    #print(tf_array.shape)\n",
    "    #print(pos_array.shape)\n",
    "    #print(oth_array.shape)\n",
    "    M = np.concatenate([tf_array, pos_array, oth_array],axis=1)\n",
    "    return pd.DataFrame(M)\n",
    "\n",
    "def get_pos_tags(tweets):\n",
    "    \"\"\"Takes a list of strings (tweets) and\n",
    "    returns a list of strings of (POS tags).\n",
    "    \"\"\"\n",
    "    tweet_tags = []\n",
    "    for t in tweets:\n",
    "        tokens = basic_tokenize(preprocess(t))\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        tag_list = [x[1] for x in tags]\n",
    "        #for i in range(0, len(tokens)):\n",
    "        tag_str = \" \".join(tag_list)\n",
    "        tweet_tags.append(tag_str)\n",
    "    return tweet_tags\n",
    "\n",
    "def get_oth_features(tweets):\n",
    "    \"\"\"Takes a list of tweets, generates features for\n",
    "    each tweet, and returns a numpy array of tweet x features\"\"\"\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment_analyzer = VS()\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 153, 1: 45}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "import os \n",
    "import re\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "\n",
    "tweets = user_tweets\n",
    "X = transform_inputs(tweets, vectorizer, tfidf, pos_vectorizer)\n",
    "y_preds = fitted_model.predict(X)\n",
    "\n",
    "unique, counts = np.unique(y_preds, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "pd.DataFrame(zip(user_tweets,y_preds)).to_csv(\"DishaPredictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
