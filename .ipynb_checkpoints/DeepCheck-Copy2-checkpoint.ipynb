{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro: DeepCheck - Smarter Gun Background Checks\n",
    "\n",
    "Present-day gun background checks process is as follows:\n",
    "## The Process of U.S. Firearm Checks\n",
    "\n",
    "1) Firearm Buyer: Fills out an ATF Form 4473 with:`name`, `age`, `address`, `place of birth`, `race`,  `citizenship`, `Social Security (optional)`, as well as the following questions:\n",
    "  - Have you ever been convicted of a felony?\n",
    "  - Have you ever been convicted of a misdemeanor crime of domestic violence?\n",
    "  - Are you an unlawful user of, or addicted to, marijuana or any other depressant, stimulant, narcotic drug, or any other controlled substance?\n",
    "  - Are you a fugitive from justice?\n",
    "  - Have you ever been committed to a mental institution?\n",
    "\n",
    "2) Firearm Seller: Submits the information to the FBI via a toll-free phone line or over the internet, and the agency checks the applicant's info against databases\n",
    "\n",
    "3) FBI: Conducts background check with the submitted form (can take minutes). The FBI will deny a claim to Fire\n",
    "\n",
    "*Source: https://www.cnn.com/2018/02/15/us/gun-background-checks-florida-school-shooting/index.html*\n",
    "\n",
    "## When is someone Denied the Right to Firearms?\n",
    "\n",
    "* Convicted of a crime punishable by imprisonmnet\n",
    "* Convicted of a violent misdemeaner\n",
    "* Addict of any controlled substance\n",
    "* Committed to a mental institution\n",
    "* Illegal immigrant\n",
    "* Harassing, stalking, or threatening an intimate partner\n",
    "* Renounced his/her US citizenship\n",
    "\n",
    "*Source: https://www.fbi.gov/services/cjis/nics/about-nics*\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset mentioned above was obtained from:\n",
    "\n",
    "Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. \"Automated Hate Speech Detection and the Problem of Offensive Language.\" Proceedings of the 11th International Conference on Web and Social Media (ICWSM).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Azure Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# IMPORT AZURE LIBRARIES\n",
    "# Azure Notebook Libraries\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "import logging\n",
    "\n",
    "# IMPORT DATA SCIENCE LIBRARIES\n",
    "import pandas as pd \n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils import resample\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\house\\Documents\\GitHub\\config.json\n"
     ]
    }
   ],
   "source": [
    "# Load workspace\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose a name for the experiment and specify the project folder.\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "experiment_name = 'hatespeech_detection'\n",
    "project_folder = './hatespeech_project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text_data.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1   0.70\n",
       "2   0.15\n",
       "0   0.05\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['class'].value_counts() / df['class'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation\n",
    "These data have over 20,000 labeled tweets in this dataset. Most tweets contain special characters and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "sentiment_analyzer = VS()\n",
    "stemmer = PorterStemmer()\n",
    "    \n",
    "tweets = df[\"tweet\"]\n",
    "\n",
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=30,\n",
    "    max_df=0.601\n",
    "    )\n",
    "\n",
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores\n",
    "\n",
    "#Get POS tags for tweets and save as a string\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    #for i in range(0, len(tokens)):\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)\n",
    "        #print(tokens[i],tag_list[i])\n",
    "\n",
    "\n",
    "# We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=30,\n",
    "    max_df=0.601,\n",
    "    )\n",
    "\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                    \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \"vader compound\", \\\n",
    "                    \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]\n",
    "feats = get_feature_array(tweets)\n",
    "X = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cloud Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"automlcl\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "    \n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\", # for GPU, use \"STANDARD_NC6\"\n",
    "                                                                #vm_priority = 'lowpriority', # optional\n",
    "                                                                max_nodes = 6)\n",
    "\n",
    "    # Create the cluster.\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# create a new RunConfig object\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute\n",
    "conda_run_config.target = compute_target\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "conda_run_config.environment.docker.base_image = azureml.core.runconfig.DEFAULT_CPU_IMAGE\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]'], conda_packages=['numpy'])\n",
    "conda_run_config.environment.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24783, 2760)\n"
     ]
    }
   ],
   "source": [
    "# Split the test train sets \n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "y = df[\"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             primary_metric = 'AUC_weighted',\n",
    "                             iteration_timeout_minutes = 60,\n",
    "                             iterations = 1,\n",
    "                             n_cross_validations = 3,\n",
    "                             verbosity = logging.INFO,\n",
    "                             X = X_train, \n",
    "                             y = y_train,\n",
    "                             path = project_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(automl_config, show_output = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>hatespeech_detection</td><td>AutoML_10e65ecb-ca40-4596-8f52-b6a4a65065b4</td><td>automl</td><td>Completed</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/a989cac4-671d-45c6-9d2b-ea7e9d936600/resourceGroups/AmherstRG/providers/Microsoft.MachineLearningServices/workspaces/AmherstWorkspace/experiments/hatespeech_detection/runs/AutoML_10e65ecb-ca40-4596-8f52-b6a4a65065b4\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: hatespeech_detection,\n",
       "Id: AutoML_10e65ecb-ca40-4596-8f52-b6a4a65065b4,\n",
       "Type: automl,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the model with log loss minimized\n",
    "lookup_metric = \"log_loss\"\n",
    "best_run, fitted_model = run.get_output(metric = lookup_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Randomly select digits and test.\n",
    "from azureml.core.model import Model\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "predicted = fitted_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score: %.2f' % accuracy_score(y_true=y_test, y_pred=predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model AutoML10e65ecbcbest\n"
     ]
    }
   ],
   "source": [
    "model = run.register_model('deepcheck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Real Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        !!! RT @mayasolovely: As a woman you shouldn't...\n",
      "1        !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
      "2        !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
      "3        !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
      "4        !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
      "5        !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...\n",
      "6        !!!!!!\"@__BrighterDays: I can not just sit up ...\n",
      "7        !!!!&#8220;@selfiequeenbri: cause I'm tired of...\n",
      "8        \" &amp; you might not get ya bitch back &amp; ...\n",
      "9        \" @rhythmixx_ :hobbies include: fighting Maria...\n",
      "10       \" Keeks is a bitch she curves everyone \" lol I...\n",
      "11                      \" Murda Gang bitch its Gang Land \"\n",
      "12       \" So hoes that smoke are losers ? \" yea ... go...\n",
      "13           \" bad bitches is the only thing that i like \"\n",
      "14                                 \" bitch get up off me \"\n",
      "15                         \" bitch nigga miss me with it \"\n",
      "16                                  \" bitch plz whatever \"\n",
      "17                               \" bitch who do you love \"\n",
      "18                      \" bitches get cut off everyday B \"\n",
      "19                      \" black bottle &amp; a bad bitch \"\n",
      "20                    \" broke bitch cant tell me nothing \"\n",
      "21                         \" cancel that bitch like Nino \"\n",
      "22                 \" cant you see these hoes wont change \"\n",
      "23       \" fuck no that bitch dont even suck dick \" &#1...\n",
      "24       \" got ya bitch tip toeing on my hardwood floor...\n",
      "25          \" her pussy lips like Heaven doors \" &#128524;\n",
      "26                            \" hoe what its hitting for \"\n",
      "27       \" i met that pussy on Ocean Dr . i gave that p...\n",
      "28          \" i need a trippy bitch who fuck on Hennessy \"\n",
      "29       \" i spend my money how i want bitch its my bus...\n",
      "                               ...                        \n",
      "24753               you ain't gotta be a dyke to like hoes\n",
      "24754                     you are a hoe, hoe, &amp; a hoe.\n",
      "24755               you bitches love yall some corny nigga\n",
      "24756    you can masturbate anytime bitch lol &#8220;@g...\n",
      "24757    you can never get a group of hoes together wit...\n",
      "24758    you can tell when dick recently been in a puss...\n",
      "24759                            you can't cuff a hoe lmao\n",
      "24760                           you drove me redneck crazy\n",
      "24761                                you fake niggah lolol\n",
      "24762                   you got niggas, and i got bitches.\n",
      "24763    you gotta be a new breed of retarded if you do...\n",
      "24764    you gotta understand that these bitches are ch...\n",
      "24765                                        you hoe spice\n",
      "24766                     you just want some attention hoe\n",
      "24767    you know what they say, the early bird gets th...\n",
      "24768    you know what your doing when you favorite a t...\n",
      "24769    you lil dumb ass bitch, i ain't fuckin wit chu...\n",
      "24770    you look like AC Green...bitch don't call here...\n",
      "24771    you look like your 12 stop talking about fucki...\n",
      "24772          you might as well gone pussy pop on a stage\n",
      "24773                you niggers cheat on ya gf's? smh....\n",
      "24774    you really care bout dis bitch. my dick all in...\n",
      "24775     you worried bout other bitches, you need me for?\n",
      "24776                                   you're all niggers\n",
      "24777    you're such a retard i hope you get type 2 dia...\n",
      "24778    you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
      "24779    you've gone and broke the wrong heart baby, an...\n",
      "24780    young buck wanna eat!!.. dat nigguh like I ain...\n",
      "24781                youu got wild bitches tellin you lies\n",
      "24782    ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
      "Name: tweet, Length: 24783, dtype: object\n",
      "0                             MAKE AMERICA GREAT AGAIN!\n",
      "1                       Good Morning, Have A Great Day!\n",
      "2     Today in Florida, @FLOTUS and I were honored t...\n",
      "3     It was announced today by the U.S. Treasury th...\n",
      "4     It is my pleasure to announce that @StephenMoo...\n",
      "5     ....There is nothing to admire about them, the...\n",
      "6     ISIS uses the internet better than almost anyo...\n",
      "7                               https://t.co/wbQiy4uGYM\n",
      "8                               https://t.co/du665IcD5H\n",
      "9     “Our own Benjamin Hall is doing fantastic repo...\n",
      "10                              https://t.co/qjoe0rc3rQ\n",
      "11       3.1 GDP FOR THE YEAR, BEST NUMBER IN 14 YEARS!\n",
      "12    RT @WhiteHouse: The American dream is back bec...\n",
      "13    RT @GOPChairwoman: You can't deny our economy ...\n",
      "14    RT @DonaldJTrumpJr: Super excited today that @...\n",
      "15    RT @WhiteHouse: President @realDonaldTrump's h...\n",
      "16    RT @WhiteHouse: Today, there are plenty of rea...\n",
      "17    Today we celebrate the lives and achievements ...\n",
      "18    We are here today to take historic action to d...\n",
      "19    After 52 years it is time for the United State...\n",
      "20    “John Solomon: As Russia Collusion fades, Ukra...\n",
      "21    “The reason we have the Special Counsel invest...\n",
      "22                              https://t.co/UhTjBuWY2h\n",
      "23    Leaving the GREAT STATE of OHIO for the @White...\n",
      "24                              https://t.co/ssJNhlTMzy\n",
      "25    ISIS Caliphate two years ago in red vs. ISIS C...\n",
      "26     Really beautiful to see! https://t.co/DKq2TIeiSs\n",
      "27    Great news from @Ford! They are investing near...\n",
      "28    I am thrilled to be here in Ohio with the hard...\n",
      "29    George Conway, often referred to as Mr. Kellya...\n",
      "                            ...                        \n",
      "70    RT @ChuckCallesto: REVEALED: Foreign Governmen...\n",
      "71    RT @ChuckCallesto: BREAKING:  ==&gt; Minnesota...\n",
      "72    RT @ChuckCallesto: Christopher Steele Admits U...\n",
      "73    RT @ChuckCallesto: President Trump Defends Jud...\n",
      "74    RT @superyayadize: NPR Accidentally Admits Bor...\n",
      "75    RT @Lrihendry: Meghan MCCain took a swipe at T...\n",
      "76    Those Republican Senators who voted in favor o...\n",
      "77    Were @FoxNews weekend anchors, @ArthelNeville ...\n",
      "78    Democrat UAW Local 1112 President David Green ...\n",
      "79                              https://t.co/yvJpMzy3R4\n",
      "80    ....to the people that got you there. Keep fig...\n",
      "81    ....must stay strong and fight back with vigor...\n",
      "82    Bring back @JudgeJeanine Pirro. The Radical Le...\n",
      "83    Happy St. Patrick’s Day! ☘️ https://t.co/WmuNz...\n",
      "84    So it was indeed (just proven in court papers)...\n",
      "85    Report: Christopher Steele backed up his Democ...\n",
      "86    ....Should Federal Election Commission and/or ...\n",
      "87    It’s truly incredible that shows like Saturday...\n",
      "88    RT @realDonaldTrump: I’d like to thank all of ...\n",
      "89    RT @realDonaldTrump: .....THIS SHOULD NEVER HA...\n",
      "90    RT @realDonaldTrump: ....should never have bee...\n",
      "91    RT @realDonaldTrump: So, if there was knowingl...\n",
      "92    RT @realDonaldTrump: “New evidence that the Ob...\n",
      "93    How is the Paris Environmental Accord working ...\n",
      "94    Google is helping China and their military, bu...\n",
      "95    Because the economy is so good, General Motors...\n",
      "96    Spreading the fake and totally discredited Dos...\n",
      "97    This is a National Emergency...\\nhttps://t.co/...\n",
      "98    RT @WhiteHouse: Sheriff Hodgson: \"Mr. Presiden...\n",
      "99    RT @WhiteHouse: Sheriff Louderback: \"The sheri...\n",
      "Name: tweet, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary package to process data in JSON format\n",
    "try:\n",
    "    import json\n",
    "except ImportError:\n",
    "    import simplejson as json\n",
    "\n",
    "# Import the tweepy library\n",
    "import tweepy\n",
    "import pprint\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Setup tweepy to authenticate with Twitter credentials:\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "ACCESS_TOKEN = '2620473211-RWN2g8omy31oxu7xu0b03fxSQ2rH2HrTMcgjAJL'\n",
    "ACCESS_SECRET = 'dySWOs8voQTlN9uaLOJNuN32v7QdQwMfNcNDCXV8O6Ku9'\n",
    "CONSUMER_KEY = '7KOV5jhvAty3QtBJurrllMggy'\n",
    "CONSUMER_SECRET = 'oSTU2ViKAuUHPZxRZ1Ha6OGv6jU8eXk5VGcc7N4wx6nxuYyH1l'\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "# Create the api to connect to twitter with your creadentials\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "\n",
    "#status_cursor = tweepy.Cursor(api.user_timeline, screen_name=\"realDonaldTrump\", count=100)\n",
    "status_cursor = tweepy.Cursor(api.user_timeline, screen_name=\"realDonaldTrump\", count=100)\n",
    "status_list = status_cursor.iterator.next()\n",
    "\n",
    "user_tweets = []\n",
    "\n",
    "for i in range(len(status_list)):\n",
    "    user_tweets += [status_list[i].text]\n",
    "\n",
    "user_df = pd.DataFrame(user_tweets,columns=['tweet'])\n",
    "\n",
    "print(df['tweet'])\n",
    "print(user_df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-9ea6e8e5cf51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_vectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-9ea6e8e5cf51>\u001b[0m in \u001b[0;36mtransform_inputs\u001b[1;34m(tweets, tf_vectorizer, idf_vector, pos_vectorizer)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mRegression\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mL1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mregularization\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mtf_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mtfidf_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_array\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0midf_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Built TF-IDF array\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1611\u001b[0m         \"\"\"\n\u001b[0;32m   1612\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1613\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1614\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1615\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmax_doc_count\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m                 raise ValueError(\n\u001b[1;32m-> 1048\u001b[1;33m                     \"max_df corresponds to < documents than min_df\")\n\u001b[0m\u001b[0;32m   1049\u001b[0m             X, self.stop_words_ = self._limit_features(X, vocabulary,\n\u001b[0;32m   1050\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "source": [
    "def transform_inputs(tweets, tf_vectorizer, idf_vector, pos_vectorizer):\n",
    "    \"\"\"\n",
    "    This function takes a list of tweets, along with used to\n",
    "    transform the tweets into the format accepted by the model.\n",
    "    Each tweet is decomposed into\n",
    "    (a) An array of TF-IDF scores for a set of n-grams in the tweet.\n",
    "    (b) An array of POS tag sequences in the tweet.\n",
    "    (c) An array of features including sentiment, vocab, and readability.\n",
    "    Returns a pandas dataframe where each row is the set of features\n",
    "    for a tweet. The features are a subset selected using a Logistic\n",
    "    Regression with L1-regularization on the training data.\n",
    "    \"\"\"\n",
    "    tf_array = tf_vectorizer.fit_transform(tweets).toarray()\n",
    "    tfidf_array = tf_array*idf_vector\n",
    "    print(\"Built TF-IDF array\")\n",
    "\n",
    "    pos_tags = get_pos_tags(tweets)\n",
    "    pos_array = pos_vectorizer.fit_transform(pos_tags).toarray()\n",
    "    print(\"Built POS array\")\n",
    "\n",
    "    oth_array = get_oth_features(tweets)\n",
    "    print(\"Built other feature array\")\n",
    "    print(tfidf_array.shape)\n",
    "    print(pos_array.shape)\n",
    "    print(oth_array.shape)\n",
    "    M = np.concatenate([tfidf_array, pos_array, oth_array],axis=1)\n",
    "    return pd.DataFrame(M)\n",
    "\n",
    "def get_pos_tags(tweets):\n",
    "    \"\"\"Takes a list of strings (tweets) and\n",
    "    returns a list of strings of (POS tags).\n",
    "    \"\"\"\n",
    "    tweet_tags = []\n",
    "    for t in tweets:\n",
    "        tokens = basic_tokenize(preprocess(t))\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        tag_list = [x[1] for x in tags]\n",
    "        #for i in range(0, len(tokens)):\n",
    "        tag_str = \" \".join(tag_list)\n",
    "        tweet_tags.append(tag_str)\n",
    "    return tweet_tags\n",
    "\n",
    "def get_oth_features(tweets):\n",
    "    \"\"\"Takes a list of tweets, generates features for\n",
    "    each tweet, and returns a numpy array of tweet x features\"\"\"\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features_(t))\n",
    "    return np.array(feats)\n",
    "\n",
    "def other_features_(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features.\n",
    "    This is modified to only include those features in the final\n",
    "    model.\"\"\"\n",
    "\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "\n",
    "    words = preprocess(tweet) #Get text only\n",
    "\n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "\n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "\n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    features = [FKRA, FRE, syllables, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "#Load ngram dict\n",
    "#Load pos dictionary\n",
    "#Load function to transform data\n",
    "\n",
    "\n",
    "X = transform_inputs(user_df, vectorizer, tfidf, pos_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "import os \n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "model=Model(ws, id='AutoML7d3a81240best:1')\n",
    "model.download(target_dir=os.getcwd(), exist_ok=True)\n",
    "\n",
    "clf = joblib.load( os.path.join(os.getcwd(), 'model.pkl'))\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "y_preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
