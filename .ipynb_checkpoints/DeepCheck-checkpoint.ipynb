{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro: DeepCheck - Smarter Gun Background Checks\n",
    "\n",
    "Present-day gun background checks process is as follows:\n",
    "## The Process of U.S. Firearm Checks\n",
    "\n",
    "1) Firearm Buyer: Fills out an ATF Form 4473 with:`name`, `age`, `address`, `place of birth`, `race`,  `citizenship`, `Social Security (optional)`, as well as the following questions:\n",
    "  - Have you ever been convicted of a felony?\n",
    "  - Have you ever been convicted of a misdemeanor crime of domestic violence?\n",
    "  - Are you an unlawful user of, or addicted to, marijuana or any other depressant, stimulant, narcotic drug, or any other controlled substance?\n",
    "  - Are you a fugitive from justice?\n",
    "  - Have you ever been committed to a mental institution?\n",
    "\n",
    "2) Firearm Seller: Submits the information to the FBI via a toll-free phone line or over the internet, and the agency checks the applicant's info against databases\n",
    "\n",
    "3) FBI: Conducts background check with the submitted form (can take minutes). The FBI will deny a claim to Fire\n",
    "\n",
    "*Source: https://www.cnn.com/2018/02/15/us/gun-background-checks-florida-school-shooting/index.html*\n",
    "\n",
    "## When is someone Denied the Right to Firearms?\n",
    "\n",
    "* Convicted of a crime punishable by imprisonmnet\n",
    "* Convicted of a violent misdemeaner\n",
    "* Addict of any controlled substance\n",
    "* Committed to a mental institution\n",
    "* Illegal immigrant\n",
    "* Harassing, stalking, or threatening an intimate partner\n",
    "* Renounced his/her US citizenship\n",
    "\n",
    "*Source: https://www.fbi.gov/services/cjis/nics/about-nics*\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset mentioned above was obtained from:\n",
    "\n",
    "Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. \"Automated Hate Speech Detection and the Problem of Offensive Language.\" Proceedings of the 11th International Conference on Web and Social Media (ICWSM).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# IMPORT AZURE LIBRARIES\n",
    "# Azure Notebook Libraries\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "import logging\n",
    "\n",
    "# IMPORT DATA SCIENCE LIBRARIES\n",
    "import pandas as pd \n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils import resample\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Azure Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\house\\Documents\\GitHub\\config.json\n"
     ]
    }
   ],
   "source": [
    "# Load workspace\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose a name for the experiment and specify the project folder.\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "experiment_name = 'hatespeech_detection'\n",
    "project_folder = './hatespeech_project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text_data.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1   0.70\n",
       "2   0.15\n",
       "0   0.05\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['class'].value_counts() / df['class'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generation\n",
    "These data have over 20,000 labeled tweets in this dataset. Most tweets contain special characters and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "sentiment_analyzer = VS()\n",
    "stemmer = PorterStemmer()\n",
    "    \n",
    "tweets = df[\"tweet\"]\n",
    "\n",
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000\n",
    "    )\n",
    "\n",
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores\n",
    "\n",
    "#Get POS tags for tweets and save as a string\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    #for i in range(0, len(tokens)):\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)\n",
    "        #print(tokens[i],tag_list[i])\n",
    "\n",
    "\n",
    "# We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    )\n",
    "\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                    \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \"vader compound\", \\\n",
    "                    \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]\n",
    "feats = get_feature_array(tweets)\n",
    "X = np.concatenate([tfidf,pos,feats],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 12668)\n"
     ]
    }
   ],
   "source": [
    "# Split the test train sets \n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "y = df[\"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             primary_metric = 'AUC_weighted',\n",
    "                             iteration_timeout_minutes = 60,\n",
    "                             iterations = 1,\n",
    "                             n_cross_validations = 3,\n",
    "                             verbosity = logging.INFO,\n",
    "                             X = X_train, \n",
    "                             y = y_train,\n",
    "                             path = project_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_run = experiment.submit(automl_config, show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-237c17c5122d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Show the model with log loss minimized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlookup_metric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"log_loss\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbest_run\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfitted_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlookup_metric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'local_run' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the model with log loss minimized\n",
    "lookup_metric = \"log_loss\"\n",
    "best_run, fitted_model = local_run.get_output(metric = lookup_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select digits and test.\n",
    "from azureml.core.model import Model\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "predicted = fitted_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score: %.2f' % accuracy_score(y_true=y_test, y_pred=predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = local_run.register_model('deepcheck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Real Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      !!! RT @mayasolovely: As a woman you shouldn't...\n",
      "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
      "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
      "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
      "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
      "5      !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...\n",
      "6      !!!!!!\"@__BrighterDays: I can not just sit up ...\n",
      "7      !!!!&#8220;@selfiequeenbri: cause I'm tired of...\n",
      "8      \" &amp; you might not get ya bitch back &amp; ...\n",
      "9      \" @rhythmixx_ :hobbies include: fighting Maria...\n",
      "10     \" Keeks is a bitch she curves everyone \" lol I...\n",
      "11                    \" Murda Gang bitch its Gang Land \"\n",
      "12     \" So hoes that smoke are losers ? \" yea ... go...\n",
      "13         \" bad bitches is the only thing that i like \"\n",
      "14                               \" bitch get up off me \"\n",
      "15                       \" bitch nigga miss me with it \"\n",
      "16                                \" bitch plz whatever \"\n",
      "17                             \" bitch who do you love \"\n",
      "18                    \" bitches get cut off everyday B \"\n",
      "19                    \" black bottle &amp; a bad bitch \"\n",
      "20                  \" broke bitch cant tell me nothing \"\n",
      "21                       \" cancel that bitch like Nino \"\n",
      "22               \" cant you see these hoes wont change \"\n",
      "23     \" fuck no that bitch dont even suck dick \" &#1...\n",
      "24     \" got ya bitch tip toeing on my hardwood floor...\n",
      "25        \" her pussy lips like Heaven doors \" &#128524;\n",
      "26                          \" hoe what its hitting for \"\n",
      "27     \" i met that pussy on Ocean Dr . i gave that p...\n",
      "28        \" i need a trippy bitch who fuck on Hennessy \"\n",
      "29     \" i spend my money how i want bitch its my bus...\n",
      "                             ...                        \n",
      "970           &#128514; bruh these hoes gotta be stupid.\n",
      "971    &#128514;&#128514; &#8220;@Juggs201: SOMEBODY ...\n",
      "972    &#128514;&#128514; &#8220;@JusDahl &#8220;@Jus...\n",
      "973    &#128514;&#128514; &#8220;@mcdonavins: Of cour...\n",
      "974    &#128514;&#128514; My stomach in pain like a m...\n",
      "975    &#128514;&#128514; bitches get stuff done. htt...\n",
      "976    &#128514;&#128514; fucc nicca yu pose to be pu...\n",
      "977    &#128514;&#128514; maaaaannnn i hope so! RT @A...\n",
      "978    &#128514;&#128514; mike calls me t-bird @mikem...\n",
      "979    &#128514;&#128514; niggas sending screenshots ...\n",
      "980    &#128514;&#128514; this white bitch think she ...\n",
      "981    &#128514;&#128514;&#128514; &#8220;@snapbackBL...\n",
      "982    &#128514;&#128514;&#128514; Nicca gotta team f...\n",
      "983    &#128514;&#128514;&#128514; RT &#8220;@TuneLuc...\n",
      "984    &#128514;&#128514;&#128514; RT @A2daO: When a ...\n",
      "985    &#128514;&#128514;&#128514; RT@gxdlyvisuals: \"...\n",
      "986             &#128514;&#128514;&#128514; bitch my lrt\n",
      "987         &#128514;&#128514;&#128514; burnt pussy lips\n",
      "988    &#128514;&#128514;&#128514; i just said that R...\n",
      "989    &#128514;&#128514;&#128514; nah if you round m...\n",
      "990    &#128514;&#128514;&#128514; played that nigga ...\n",
      "991    &#128514;&#128514;&#128514;&#128514; &#128128;...\n",
      "992    &#128514;&#128514;&#128514;&#128514; @shoota10...\n",
      "993    &#128514;&#128514;&#128514;&#128514; I hate yo...\n",
      "994    &#128514;&#128514;&#128514;&#128514; RT @MREEU...\n",
      "995    &#128514;&#128514;&#128514;&#128514; RT @SMASH...\n",
      "996    &#128514;&#128514;&#128514;&#128514; bitch if ...\n",
      "997    &#128514;&#128514;&#128514;&#128514; these fol...\n",
      "998    &#128514;&#128514;&#128514;&#128514;&#128514; ...\n",
      "999    &#128514;&#128514;&#128514;&#128514;&#128514;&...\n",
      "Name: tweet, Length: 1000, dtype: object\n",
      "0      Today in Florida, @FLOTUS and I were honored t...\n",
      "1      It was announced today by the U.S. Treasury th...\n",
      "2      It is my pleasure to announce that @StephenMoo...\n",
      "3      ....There is nothing to admire about them, the...\n",
      "4      ISIS uses the internet better than almost anyo...\n",
      "5                                https://t.co/wbQiy4uGYM\n",
      "6                                https://t.co/du665IcD5H\n",
      "7      “Our own Benjamin Hall is doing fantastic repo...\n",
      "8                                https://t.co/qjoe0rc3rQ\n",
      "9         3.1 GDP FOR THE YEAR, BEST NUMBER IN 14 YEARS!\n",
      "10     RT @WhiteHouse: The American dream is back bec...\n",
      "11     RT @GOPChairwoman: You can't deny our economy ...\n",
      "12     RT @DonaldJTrumpJr: Super excited today that @...\n",
      "13     RT @WhiteHouse: President @realDonaldTrump's h...\n",
      "14     RT @WhiteHouse: Today, there are plenty of rea...\n",
      "15     Today we celebrate the lives and achievements ...\n",
      "16     We are here today to take historic action to d...\n",
      "17     After 52 years it is time for the United State...\n",
      "18     “John Solomon: As Russia Collusion fades, Ukra...\n",
      "19     “The reason we have the Special Counsel invest...\n",
      "20                               https://t.co/UhTjBuWY2h\n",
      "21     Leaving the GREAT STATE of OHIO for the @White...\n",
      "22                               https://t.co/ssJNhlTMzy\n",
      "23     ISIS Caliphate two years ago in red vs. ISIS C...\n",
      "24      Really beautiful to see! https://t.co/DKq2TIeiSs\n",
      "25     Great news from @Ford! They are investing near...\n",
      "26     I am thrilled to be here in Ohio with the hard...\n",
      "27     George Conway, often referred to as Mr. Kellya...\n",
      "28     The Democrats are getting very “strange.” They...\n",
      "29         Not a good situation! https://t.co/uaMcSrX4yM\n",
      "                             ...                        \n",
      "170    Republican Senators have a very easy vote this...\n",
      "171    RT @RepAndyBiggsAZ: Kate Steinle.\\nSarah Root....\n",
      "172    RT @GOPChairwoman: .@realDonaldTrump made hist...\n",
      "173    Making Daylight Saving Time permanent is O.K. ...\n",
      "174    At a recent round table meeting of business ex...\n",
      "175    RT @ChuckRossDC: NEW: Dark money group gave $2...\n",
      "176    RT @TomFitton: BIG:@JudicialWatch Uncovers DOJ...\n",
      "177    RT @USAmbIsrael: With @LindseyGrahamSC today t...\n",
      "178    RT @LindseyGrahamSC: Executive Business meetin...\n",
      "179    RT @senatemajldr: H.R.1 is a blatant power gra...\n",
      "180    RT @paulsperry_: BREAKING: US Border Patrol sa...\n",
      "181    RT @paulsperry_: BREAKING: Even Mueller's case...\n",
      "182    RT @TomFitton: The real collusion scandal... w...\n",
      "183    RT @TomFitton: Top House Dem says Cohen likely...\n",
      "184    RT @TomFitton: .@RepAdamSchiff has another eth...\n",
      "185    “There’s not one shred of evidence that Presid...\n",
      "186    RT @paulsperry_: If Schiff wasn't coaching Coh...\n",
      "187    More people are working today in the United St...\n",
      "188    Despite the most hostile and corrupt media in ...\n",
      "189    RT @WhiteHouse: “Public optimism in their pers...\n",
      "190    RT @mike_pence: Great to be in Kentucky! I’m p...\n",
      "191    “Donald Trump’s Approval Rating is at or near ...\n",
      "192    RT @TomFitton: .@JudicialWatch uncovers major ...\n",
      "193    RT @WhiteHouse: Economic security is national ...\n",
      "194    RT @GOPChairwoman: Since the mainstream media ...\n",
      "195     Will soon be 145 Judges! https://t.co/LoTbT4RFJj\n",
      "196    This is just the beginning! https://t.co/PYwFG...\n",
      "197    RT @TimRunsHisMouth: Democrats so far in 2019:...\n",
      "198    The Witch Hunt continues! https://t.co/9W1iUgE0d6\n",
      "199    ....and renovated, with MUCH MORE to follow sh...\n",
      "Name: tweet, Length: 200, dtype: object\n",
      "(1, 19)\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary package to process data in JSON format\n",
    "try:\n",
    "    import json\n",
    "except ImportError:\n",
    "    import simplejson as json\n",
    "\n",
    "# Import the tweepy library\n",
    "import tweepy\n",
    "import pprint\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Setup tweepy to authenticate with Twitter credentials:\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "# Create the api to connect to twitter with your creadentials\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "\n",
    "#status_cursor = tweepy.Cursor(api.user_timeline, screen_name=\"realDonaldTrump\", count=100)\n",
    "status_cursor = tweepy.Cursor(api.user_timeline, screen_name=\"realDonaldTrump\", count=1000)\n",
    "status_list = status_cursor.iterator.next()\n",
    "\n",
    "user_tweets = []\n",
    "\n",
    "for i in range(len(status_list)):\n",
    "    user_tweets += [status_list[i].text]\n",
    "\n",
    "model = joblib.load('final_model.pkl')\n",
    "\n",
    "user_df = pd.DataFrame(user_tweets,columns=['tweet'])\n",
    "\n",
    "print(df['tweet'])\n",
    "print(user_df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built TF-IDF array\n",
      "Built POS array\n",
      "Built other feature array\n",
      "(24783, 10000)\n",
      "(24783, 5000)\n",
      "(24783, 11)\n"
     ]
    }
   ],
   "source": [
    "def transform_inputs(tweets, tf_vectorizer, idf_vector, pos_vectorizer):\n",
    "    \"\"\"\n",
    "    This function takes a list of tweets, along with used to\n",
    "    transform the tweets into the format accepted by the model.\n",
    "    Each tweet is decomposed into\n",
    "    (a) An array of TF-IDF scores for a set of n-grams in the tweet.\n",
    "    (b) An array of POS tag sequences in the tweet.\n",
    "    (c) An array of features including sentiment, vocab, and readability.\n",
    "    Returns a pandas dataframe where each row is the set of features\n",
    "    for a tweet. The features are a subset selected using a Logistic\n",
    "    Regression with L1-regularization on the training data.\n",
    "    \"\"\"\n",
    "    tf_array = tf_vectorizer.fit_transform(tweets).toarray()\n",
    "    tfidf_array = tf_array*idf_vector\n",
    "    print(\"Built TF-IDF array\")\n",
    "\n",
    "    pos_tags = get_pos_tags(tweets)\n",
    "    pos_array = pos_vectorizer.fit_transform(pos_tags).toarray()\n",
    "    print(\"Built POS array\")\n",
    "\n",
    "    oth_array = get_oth_features(tweets)\n",
    "    print(\"Built other feature array\")\n",
    "    print(tfidf_array.shape)\n",
    "    print(pos_array.shape)\n",
    "    print(oth_array.shape)\n",
    "    M = np.concatenate([tfidf_array, pos_array, oth_array],axis=1)\n",
    "    return pd.DataFrame(M)\n",
    "\n",
    "def get_pos_tags(tweets):\n",
    "    \"\"\"Takes a list of strings (tweets) and\n",
    "    returns a list of strings of (POS tags).\n",
    "    \"\"\"\n",
    "    tweet_tags = []\n",
    "    for t in tweets:\n",
    "        tokens = basic_tokenize(preprocess(t))\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        tag_list = [x[1] for x in tags]\n",
    "        #for i in range(0, len(tokens)):\n",
    "        tag_str = \" \".join(tag_list)\n",
    "        tweet_tags.append(tag_str)\n",
    "    return tweet_tags\n",
    "\n",
    "def get_oth_features(tweets):\n",
    "    \"\"\"Takes a list of tweets, generates features for\n",
    "    each tweet, and returns a numpy array of tweet x features\"\"\"\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features_(t))\n",
    "    return np.array(feats)\n",
    "\n",
    "def other_features_(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features.\n",
    "    This is modified to only include those features in the final\n",
    "    model.\"\"\"\n",
    "\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "\n",
    "    words = preprocess(tweet) #Get text only\n",
    "\n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "\n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "\n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    features = [FKRA, FRE, syllables, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "#Load ngram dict\n",
    "#Load pos dictionary\n",
    "#Load function to transform data\n",
    "\n",
    "\n",
    "X = transform_inputs(tweets, vectorizer, tfidf, pos_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\house\\Documents\\GitHub\\config.json\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (330,12668) (11166,) (330,12668) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-7228092df920>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0my_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    971\u001b[0m             \u001b[0minplace_column_scale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (330,12668) (11166,) (330,12668) "
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "import os \n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "model=Model(ws, id='AutoML7d3a81240best:1')\n",
    "model.download(target_dir=os.getcwd(), exist_ok=True)\n",
    "\n",
    "clf = joblib.load( os.path.join(os.getcwd(), 'model.pkl'))\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "y_preds = model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
