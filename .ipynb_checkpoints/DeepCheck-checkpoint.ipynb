{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro: DeepCheck - Smarter Gun Background Checks\n",
    "\n",
    "Present-day gun background checks process is as follows:\n",
    "## The Process of U.S. Firearm Checks\n",
    "\n",
    "1) Firearm Buyer: Fills out an ATF Form 4473 with:`name`, `age`, `address`, `place of birth`, `race`,  `citizenship`, `Social Security (optional)`, as well as the following questions:\n",
    "  - Have you ever been convicted of a felony?\n",
    "  - Have you ever been convicted of a misdemeanor crime of domestic violence?\n",
    "  - Are you an unlawful user of, or addicted to, marijuana or any other depressant, stimulant, narcotic drug, or any other controlled substance?\n",
    "  - Are you a fugitive from justice?\n",
    "  - Have you ever been committed to a mental institution?\n",
    "\n",
    "2) Firearm Seller: Submits the information to the FBI via a toll-free phone line or over the internet, and the agency checks the applicant's info against databases\n",
    "\n",
    "3) FBI: Conducts background check with the submitted form (can take minutes). The FBI will deny a claim to Fire\n",
    "\n",
    "*Source: https://www.cnn.com/2018/02/15/us/gun-background-checks-florida-school-shooting/index.html*\n",
    "\n",
    "## When is someone Denied the Right to Firearms?\n",
    "\n",
    "* Convicted of a crime punishable by imprisonmnet\n",
    "* Convicted of a violent misdemeaner\n",
    "* Addict of any controlled substance\n",
    "* Committed to a mental institution\n",
    "* Illegal immigrant\n",
    "* Harassing, stalking, or threatening an intimate partner\n",
    "* Renounced his/her US citizenship\n",
    "\n",
    "*Source: https://www.fbi.gov/services/cjis/nics/about-nics*\n",
    "\n",
    "## The Purpose of DeepCheck\n",
    "Deepcheck was made to anals\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset mentioned above was obtained from:\n",
    "\n",
    "Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. \"Automated Hate Speech Detection and the Problem of Offensive Language.\" Proceedings of the 11th International Conference on Web and Social Media (ICWSM).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# IMPORT AZURE LIBRARIES\n",
    "# Azure Notebook Libraries\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "import logging\n",
    "\n",
    "# IMPORT DATA SCIENCE LIBRARIES\n",
    "import pandas as pd \n",
    "import scipy\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Azure Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: C:\\Users\\house\\Documents\\GitHub\\config.json\n"
     ]
    }
   ],
   "source": [
    "# Load workspace\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the experiment and specify the project folder.\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "experiment_name = 'hatespeech_detection'\n",
    "project_folder = './hatespeech_project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text_data.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class imbalance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1   0.80\n",
       "0   0.10\n",
       "Name: new_class, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print('class imbalance:')\n",
    "df['new_class'].value_counts() / df['class'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this class imbalance, we chose to upsample the class 0 (innoculus tweets) to match class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbanalnced Class Frequencies:\n",
      "1   0.88\n",
      "0   0.12\n",
      "Name: new_class, dtype: float64\n",
      "First count: 24783\n",
      "\n",
      "New Class Frequencies:\n",
      "1   0.50\n",
      "0   0.50\n",
      "Name: new_class, dtype: float64\n",
      "Updated count: 5744\n"
     ]
    }
   ],
   "source": [
    "print('Imbanalnced Class Frequencies:')\n",
    "print(df['new_class'].value_counts() / df['new_class'].count())\n",
    "print('First count: %i' % df['class'].count())\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_0 = df[df['new_class']==0]\n",
    "df_1 = df[df['new_class']==1]\n",
    "\n",
    "# Number of observations in majority class\n",
    "i_class1 = np.where(df['new_class'] == 0)[0]\n",
    "n_class1 = int(round(len(i_class1),0))\n",
    "\n",
    "# Upsample to match classes\n",
    "df_0_upsampled = resample(df_1, \n",
    "                            replace=False,           # sample with replacement\n",
    "                            n_samples=n_class1,     # to match minority class\n",
    "                            random_state=123)       # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_0_upsampled, df_0])\n",
    " \n",
    "# Display new class counts\n",
    "print('\\nNew Class Frequencies:')\n",
    "print(df_downsampled['new_class'].value_counts() / df_downsampled['new_class'].count())\n",
    "print('Updated count: %i' % df_downsampled['new_class'].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extration\n",
    "These data have over 20,000 labeled tweets in this dataset. Most tweets contain special characters and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_downsampled[\"tweet\"]\n",
    "import feature_extraction\n",
    "\n",
    "X, vectorizer, tfidf, pos_vectorizer = feature_extraction.get_features(tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cloud Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data to Datastore\n",
    "The training data must be uploaded to the datastore in order to compute the training on the cloud. To do this, the training data will be downloaded locally to a `.tsv` file and then uploaded to the datastore with the `ds.upload()` command. Uploading to the datastore is a one-time task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the data in a temporary folder\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "# store the get_data() script in the project folder\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the test train sets \n",
    "y = df_downsampled[\"new_class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).to_csv(\"data/X_train.tsv\", index=False, header=False, quoting=csv.QUOTE_ALL, sep=\"\\t\")\n",
    "pd.DataFrame(y_train).to_csv(\"data/y_train.tsv\", index=False, header=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ./data\\X_train.tsv\n",
      "Uploading ./data\\y_train.tsv\n",
      "Uploaded ./data\\y_train.tsv, 1 files out of an estimated total of 2\n",
      "Uploaded ./data\\X_train.tsv, 2 files out of an estimated total of 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_aef24cf01c0e47adb29ebe0d34af7878"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir='./data', target_path='hatespeech_data', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "dr = DataReferenceConfiguration(datastore_name=ds.name, \n",
    "                   path_on_datastore='hatespeech_data', \n",
    "                   path_on_compute='/tmp/azureml_runs',\n",
    "                   mode='download', # download files from datastore to compute target\n",
    "                   overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model with Cloud Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"deepcheck\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "    \n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\",\n",
    "                                                                max_nodes = 6)\n",
    "\n",
    "    # Create the cluster.\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# create a new RunConfig object\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute\n",
    "conda_run_config.target = compute_target\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "conda_run_config.environment.docker.base_image = azureml.core.runconfig.DEFAULT_CPU_IMAGE\n",
    "\n",
    "# set the data reference of the run coonfiguration\n",
    "conda_run_config.data_references = {ds.name: dr}\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]'], conda_packages=['numpy','py-xgboost<=0.80'])\n",
    "conda_run_config.environment.python.conda_dependencies = cd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./hatespeech_project/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_folder/get_data.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_data():\n",
    "    X_train = pd.read_csv(\"/tmp/azureml_runs/hatespeech_data/X_train.tsv\", delimiter=\"\\t\", header=None, quotechar='\"')\n",
    "    y_train = pd.read_csv(\"/tmp/azureml_runs/hatespeech_data/y_train.tsv\", delimiter=\"\\t\", header=None, quotechar='\"')\n",
    "\n",
    "    return { \"X\" : X_train.values, \"y\" : y_train[0].values }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\": 10,\n",
    "    \"iterations\": 10,\n",
    "    \"n_cross_validations\": 3,\n",
    "    \"primary_metric\": 'AUC_weighted',\n",
    "    \"verbosity\": logging.INFO\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             path = project_folder,\n",
    "                             run_configuration=conda_run_config,\n",
    "                             data_script = project_folder + \"/get_data.py\",\n",
    "                             **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on remote compute: deepcheck\n",
      "Parent Run ID: AutoML_c7056a49-84c9-4cb9-a1ff-44e2aed23d34\n",
      "********************************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "SAMPLING %: Percent of the training data to sample.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "********************************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       SAMPLING %  DURATION      METRIC      BEST\n",
      "         0   MaxAbsScaler LightGBM                          100.0000    0:01:20       0.9053    0.9053\n",
      "         1   RobustScaler LightGBM                          100.0000    0:01:19       0.9588    0.9588\n",
      "         2   RobustScaler LogisticRegression                100.0000    0:01:19       0.9655    0.9655\n",
      "         3   StandardScalerWrapper LightGBM                 100.0000    0:01:20       0.8580    0.9655\n",
      "         4   MaxAbsScaler LightGBM                          100.0000    0:01:19       0.9402    0.9655\n"
     ]
    }
   ],
   "source": [
    "run = experiment.submit(automl_config, show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>hatespeech_detection</td><td>AutoML_c4e11dd0-cf4c-4dd0-9fb6-64931fa5dd88</td><td>automl</td><td>Completed</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/a989cac4-671d-45c6-9d2b-ea7e9d936600/resourceGroups/AmherstRG/providers/Microsoft.MachineLearningServices/workspaces/AmherstWorkspace/experiments/hatespeech_detection/runs/AutoML_c4e11dd0-cf4c-4dd0-9fb6-64931fa5dd88\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: hatespeech_detection,\n",
       "Id: AutoML_c4e11dd0-cf4c-4dd0-9fb6-64931fa5dd88,\n",
       "Type: automl,\n",
       "Status: Completed)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the model with log loss minimized\n",
    "#lookup_metric = \"log_loss\"\n",
    "best_run, fitted_model = run.get_output(iteration=None, metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 968, 1: 928}\n",
      "Accuracy score: 0.93\n"
     ]
    }
   ],
   "source": [
    "# Randomly select digits and test.\n",
    "from azureml.core.model import Model\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "predicted = fitted_model.predict(X_test)\n",
    "\n",
    "unique, counts = np.unique(predicted, return_counts=True)\n",
    "\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy score: %.2f' % accuracy_score(y_true=y_test, y_pred=predicted))\n",
    "\n",
    "pd.DataFrame(zip(y_test,predicted)).to_csv(\"predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model AutoMLc4e11dd0cbest\n"
     ]
    }
   ],
   "source": [
    "model = run.register_model('deepcheck')\n",
    "# Best model is: AutoML54f661779best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Real Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Trump flags being waived at the Bibi @Netanyah...\n",
      "1      RT @JudicialWatch: Judicial Watch President @T...\n",
      "2      RT @JudicialWatch: BREAKING: JW announced toda...\n",
      "3      Everybody is now acknowledging that, right fro...\n",
      "4      RT @IWV: .@POTUS @realDonaldTrump has \"deliver...\n",
      "5      RT @HeyTammyBruce: Trump Job Approval Jumps to...\n",
      "6      The Democrats must end the loopholes on immigr...\n",
      "7      “The underlying issue remains the same without...\n",
      "8       THANK YOU, WORKING HARD! https://t.co/3OUESGRw43\n",
      "9        Check this out - TRUTH! https://t.co/2HNVeEpKDu\n",
      "10     Today, it was my great honor to welcome Presid...\n",
      "11     “She (Congresswoman Omar) keeps on assaulting ...\n",
      "12                               https://t.co/OIpp7mL2uD\n",
      "13     “What’s completely unacceptable is for Congess...\n",
      "14     On National Former Prisoner of War Recognition...\n",
      "15     The Mainstream Media has never been more inacc...\n",
      "16     ....I got along very well with Jerry during th...\n",
      "17     Congressman Jerry Nadler fought me for years o...\n",
      "18     The World Trade Organization finds that the Eu...\n",
      "19     RT @GOPChairwoman: Our economy is on fire.\\n \\...\n",
      "20     RT @WhiteHouse: The newly constructed 30-foot ...\n",
      "21             Congratulations to Virginia - Great game!\n",
      "22     RT @TeamTrump: BIGGER paychecks under Presiden...\n",
      "23     A 9th Circuit Judge just ruled that Mexico is ...\n",
      "24     Congratulations to the Baylor Lady Bears on th...\n",
      "25     The Democrats will never be satisfied, no matt...\n",
      "26     Uganda must find the kidnappers of the America...\n",
      "27     RT @cspan: Rep. @Jim_Jordan on President Trump...\n",
      "28     RT @Jim_Jordan: Dems want President’s tax retu...\n",
      "29     RT @Jim_Jordan: Dem talk:\\n-Abolish ICE\\n-Bord...\n",
      "                             ...                        \n",
      "170    The Mainstream Media is under fire and being s...\n",
      "171    RT @TomFitton: Tom Fitton on @RealDonaldTrump ...\n",
      "172    RT @DonaldJTrumpJr: Christmas came early this ...\n",
      "173    RT @ZacharyIvanPor1: @realDonaldTrump Thank yo...\n",
      "174    RT @ScottABC7: The Stanley Cup and the Caps wi...\n",
      "175    A team of great champions! https://t.co/Pm5p9L...\n",
      "176    RT @keeperofthecup: Hanging out in the Rooseve...\n",
      "177    RT @bmcnally14: The #Caps won a Stanley Cup so...\n",
      "178    WSJ: Obama Admin Must Account for ‘Abuse of Su...\n",
      "179    Today, it was my great honor to welcome Prime ...\n",
      "180                              https://t.co/G4RNXzoWqc\n",
      "181                              https://t.co/DAT0cT72WX\n",
      "182    RT @realDonaldTrump: Good Morning, Have A Grea...\n",
      "183    RT @realDonaldTrump: No Collusion, No Obstruct...\n",
      "184       RT @realDonaldTrump: MAKE AMERICA GREAT AGAIN!\n",
      "185    “Breaking News: Mueller Report Finds No Trump-...\n",
      "186    “The Special Counsel did not find that the Tru...\n",
      "187    “No matter your ideologies or your loyalties, ...\n",
      "188    No Collusion, No Obstruction, Complete and Tot...\n",
      "189                            MAKE AMERICA GREAT AGAIN!\n",
      "190                      Good Morning, Have A Great Day!\n",
      "191    Today in Florida, @FLOTUS and I were honored t...\n",
      "192    It was announced today by the U.S. Treasury th...\n",
      "193    It is my pleasure to announce that @StephenMoo...\n",
      "194    ....There is nothing to admire about them, the...\n",
      "195    ISIS uses the internet better than almost anyo...\n",
      "196                              https://t.co/wbQiy4uGYM\n",
      "197                              https://t.co/du665IcD5H\n",
      "198    “Our own Benjamin Hall is doing fantastic repo...\n",
      "199                              https://t.co/qjoe0rc3rQ\n",
      "Name: tweet, Length: 200, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary package to process data in JSON format\n",
    "try:\n",
    "    import json\n",
    "except ImportError:\n",
    "    import simplejson as json\n",
    "\n",
    "# Import the tweepy library\n",
    "import tweepy\n",
    "import pprint\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Setup tweepy to authenticate with Twitter credentials:\n",
    "# Variables that contains the user credentials to access Twitter API \n",
    "ACCESS_TOKEN = '2620473211-RWN2g8omy31oxu7xu0b03fxSQ2rH2HrTMcgjAJL'\n",
    "ACCESS_SECRET = 'dySWOs8voQTlN9uaLOJNuN32v7QdQwMfNcNDCXV8O6Ku9'\n",
    "CONSUMER_KEY = '7KOV5jhvAty3QtBJurrllMggy'\n",
    "CONSUMER_SECRET = 'oSTU2ViKAuUHPZxRZ1Ha6OGv6jU8eXk5VGcc7N4wx6nxuYyH1l'\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "# Create the api to connect to twitter with your creadentials\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "\n",
    "#status_cursor = tweepy.Cursor(api.user_timeline, screen_name=\"realDonaldTrump\", count=100)\n",
    "status_cursor = tweepy.Cursor(api.user_timeline, screen_name=\"realDonaldTrump\", count=1000)\n",
    "status_list = status_cursor.iterator.next()\n",
    "\n",
    "user_tweets = []\n",
    "\n",
    "for i in range(len(status_list)):\n",
    "    user_tweets += [status_list[i].text]\n",
    "\n",
    "user_df = pd.DataFrame(user_tweets,columns=['tweet'])\n",
    "\n",
    "print(user_df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inputs(tweets, tf_vectorizer, idf_vector, pos_vectorizer):\n",
    "    tf_array = tf_vectorizer.transform(tweets).toarray()\n",
    "    #print(tf_array.shape)\n",
    "    #tfidf_array = tf_array*idf_vector\n",
    "    #print(\"Built TF-IDF array\")\n",
    "\n",
    "    tweet_tags = get_pos_tags(tweets)\n",
    "    pos_array = pos_vectorizer.transform(pd.Series(tweet_tags)).toarray()\n",
    "    #print(\"Built POS array\")\n",
    "    \n",
    "    oth_array = get_feature_array(tweets)\n",
    "    #print(\"Built other feature array\")\n",
    "    #print(tf_array.shape)\n",
    "    #print(pos_array.shape)\n",
    "    #print(oth_array.shape)\n",
    "    M = np.concatenate([tf_array, pos_array, oth_array],axis=1)\n",
    "    return pd.DataFrame(M)\n",
    "\n",
    "def get_pos_tags(tweets):\n",
    "    \"\"\"Takes a list of strings (tweets) and\n",
    "    returns a list of strings of (POS tags).\n",
    "    \"\"\"\n",
    "    tweet_tags = []\n",
    "    for t in tweets:\n",
    "        tokens = basic_tokenize(preprocess(t))\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        tag_list = [x[1] for x in tags]\n",
    "        #for i in range(0, len(tokens)):\n",
    "        tag_str = \" \".join(tag_list)\n",
    "        tweet_tags.append(tag_str)\n",
    "    return tweet_tags\n",
    "\n",
    "def get_oth_features(tweets):\n",
    "    \"\"\"Takes a list of tweets, generates features for\n",
    "    each tweet, and returns a numpy array of tweet x features\"\"\"\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-2a839bac808b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#user_df = pd.DataFrame(user_tweets,columns=['tweet'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_vectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0my_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfitted_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#print(tweets)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "import os \n",
    "\n",
    "#ws = Workspace.from_config()\n",
    "\n",
    "#model=Model(ws, id='AutoML7d3a81240best:1')\n",
    "#model.download(target_dir=os.getcwd(), exist_ok=True)\n",
    "\n",
    "#clf = joblib.load( os.path.join(os.getcwd(), 'model.pkl'))\n",
    "#y_hat = clf.predict(X_test)\n",
    "\n",
    "#user_df = pd.DataFrame(user_tweets,columns=['tweet'])\n",
    "tweets = user_tweets\n",
    "X = transform_inputs(tweets, vectorizer, tfidf, pos_vectorizer)\n",
    "y_preds = fitted_model.predict(X)\n",
    "#print(tweets)\n",
    "#print(y_preds)\n",
    "\n",
    "#pd.DataFrame(zip(tweets,y_preds)).to_csv(\"data/X_train.csv\")\n",
    "\n",
    "unique, counts = np.unique(y_preds, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "pd.DataFrame(zip(user_tweets,y_preds)).to_csv(\"trump_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
